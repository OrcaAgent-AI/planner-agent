"""Planner Agent implementation with step-by-step execution.

This module implements a planning agent that can break down complex tasks into
executable steps and dynamically adjust the plan based on execution results.

The agent follows a three-stage workflow:
1. Plan: Generate an initial execution plan based on user input
2. Execute: Execute individual steps from the plan
3. Replan: Adjust the plan based on execution results or provide final response

Key Components:
- plan_step: Generates initial execution plan using LLM
- execute_step: Executes individual plan steps
- replan_step: Dynamically adjusts plan or provides final response
- should_end: Determines workflow termination condition

The agent uses structured outputs (Plan, Act, Response) to ensure consistent
and parseable responses from the LLM.
"""

from datetime import UTC, datetime
from tkinter import NO

from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import END, START, StateGraph
from langgraph.runtime import Runtime
from langgraph.prebuilt import create_react_agent

from planner_agent.context import Context
from planner_agent.prompts import (
    EXECUTION_PROMPT,
    PLANNER_PROMPT,
    REPLANNER_PROMPT,
    SYSTEM_PROMPT,
)
from planner_agent.state import State, InputState
from planner_agent.tools_and_schemas import Act, Plan, Response
from planner_agent.utils import get_message_text, load_chat_model
from planner_agent.tools import get_mcp_tools, get_tools


# Define global variables to cache and reuse LLM agents across steps,
# preventing unnecessary re-instantiation during workflow execution.
planner = None      # Agent for generating the initial plan
replanner = None    # Agent for plan adjustment or reflection
executor = None     # Agent for executing individual steps

async def plan_step(
    state: State, runtime: Runtime[Context]
) -> State:
    """Generate execution plan based on user input.
    
    This function extracts the user's input from the conversation messages and
    uses an LLM to generate a structured execution plan. The plan consists of
    individual steps that, when executed in sequence, will accomplish the user's goal.
    
    Args:
        state: Current execution state containing conversation messages
        runtime: Runtime context providing access to model configurations
        
    Returns:
        Updated state containing:
        - input: Extracted user input text
        - plan: List of execution steps generated by the LLM
        - initial_plan: Copy of the original plan for reference
        - past_steps: Empty list to track completed steps
        
    Raises:
        ValueError: If no user message is found in the conversation state
    """
    global planner
    input_text = get_message_text(state.messages[-1])
    
    # Create planner
    planner_llm = load_chat_model(runtime.context.plan_model)
    if not planner:
        planner = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", PLANNER_PROMPT)
        ]) | planner_llm.with_structured_output(Plan)
    
    plan = await planner.ainvoke({"input": input_text, "system_time": datetime.now(tz=UTC).isoformat()})
    plan_str = "\n".join(
        f"{step}" for i, step in enumerate(state.plan)
    )
    return {
        "input": input_text,
        "plan": plan.steps,
        "initial_plan": plan.steps,
        "messages": [AIMessage(content=plan_str)],
        "past_steps": []
    }

async def execute_step(
    state: State, runtime: Runtime[Context]
) -> State:
    """Execute a single step from the current execution plan.
    
    This function takes the first step from the current plan and executes it using
    an LLM. The execution context includes the full plan, the specific task to
    execute, and any previously completed steps for context.
    
    Args:
        state: Current execution state containing the plan and past steps
        runtime: Runtime context providing access to model configurations
        
    Returns:
        Updated state containing:
        - past_steps: List of tuples (task, result) for completed steps
        
    Note:
        The function filters out SELECT statements from the plan string to avoid
        exposing sensitive database queries in the execution context.
    """
    global executor
    plan_str = "\n".join(
        f"{step}" for i, step in enumerate(state.plan)
    )
    task = state.plan[0]
    task_formatted = EXECUTION_PROMPT.format(
        plan_str=plan_str,
        task=task,
        past_steps=state.past_steps
    )

    # Format the system prompt. Customize this to change the agent's behavior.
    system_message = runtime.context.system_prompt.format(
        system_time=datetime.now(tz=UTC).isoformat()
    )
    execute_llm = load_chat_model(runtime.context.execute_model)
    
    # Load tools asynchronously
    tools = await get_tools()
    if not executor:
        executor = create_react_agent(
            model=execute_llm,
            tools=tools,
            version="v2",
            prompt=system_message
        )
    response = await executor.ainvoke(
        {"messages": [HumanMessage(content=task_formatted)]}
    )

    return {
        "past_steps": [(task, response["messages"][-1].content)],
        "messages": [AIMessage(content=plan_str)]
    }

async def replan_step(
    state: State, runtime: Runtime[Context]
) -> State:
    """Replan execution steps based on current progress.
    
    This function evaluates the current execution state and determines whether to:
    1. Continue with remaining steps from the original plan
    2. Provide a final response if the objective has been achieved
    
    The replanner uses an LLM to analyze the original objective, current plan,
    and completed steps to make this decision.
    
    Args:
        state: Current execution state containing input, plan, and past steps
        runtime: Runtime context providing access to model configurations
        
    Returns:
        Updated state containing either:
        - response: Final response message if objective is complete
        - messages: AIMessage with the final response
        OR
        - plan: Updated list of remaining steps to execute
        
    Note:
        The replanner uses structured output (Act) to ensure consistent decision
        making between continuing execution or providing a final response.
    """
    global replanner
    # Create replanner
    replanner_llm = load_chat_model(runtime.context.replan_model)
    if not replanner:
        replanner = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", REPLANNER_PROMPT)
        ]) | replanner_llm.with_structured_output(Act)
    
    output = await replanner.ainvoke({
        "input": state.input,
        "plan": state.plan,
        "past_steps": state.past_steps,
        "system_time": datetime.now(tz=UTC).isoformat()
    })

    if isinstance(output.action, Response):
        return {
            "response": output.action.response,
            "messages": [AIMessage(content=output.action.response)]
        }
    else:
        return {
            "plan": output.action.steps,
        }

def should_end(state: State) -> str:
    """Determine whether to end execution based on current state.
    
    This function serves as a conditional edge function in the workflow graph.
    It checks if a final response has been generated, indicating that the
    user's objective has been completed.
    
    Args:
        state: Current execution state to evaluate
        
    Returns:
        str: Either "END" to terminate the workflow or "execute" to continue execution with the next step
    """
    if state.response:
        return END
    else:
        return "execute"


# Create the main workflow graph
workflow = StateGraph(State, input_schema=InputState, context_schema=Context)

# Add workflow nodes
workflow.add_node("planner", plan_step)      # Generate initial execution plan
workflow.add_node("execute", execute_step)     # Execute individual plan steps
workflow.add_node("replan", replan_step)     # Evaluate progress and replan

# Define workflow edges and flow control
workflow.add_edge(START, "planner")          # Start with planning
workflow.add_edge("planner", "execute")        # Execute after planning
workflow.add_edge("execute", "replan")         # Replan after execution
workflow.add_conditional_edges(              # Conditional routing from replan
    "replan",
    should_end,                              # Decision function
    ["execute", END],                          # Continue execution or terminate
)

# Compile the workflow into an executable agent
graph = workflow.compile(name="Planner Agent")